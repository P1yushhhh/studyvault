"""
File Utility Module - Recursive directory scanning and file type detection.

Provides utilities for scanning directories recursively, detecting supported file types,
and preventing duplicate processing. Uses DFS traversal with deduplication via set.
Supports parallel scanning for large directory trees.
"""

from pathlib import Path
from typing import List, Set
from concurrent.futures import ThreadPoolExecutor
import logging
import stat as stat_module

from studyvault.utils.logger import get_logger

logger = get_logger(__name__)


class FileUtil:
    """
    Static utility class for file operations.
    
    Supports recursive directory scanning with deduplication for file types:
    - Text files: .txt, .md
    - Documents: .pdf, .docx, .pptx
    - Audio: .mp3
    - Video: .mp4
    """
    
    SUPPORTED_EXTENSIONS = {'.txt', '.md', '.pdf', '.mp3', '.mp4', '.docx', '.pptx'}
    
    TYPE_MAPPING = {
        '.txt': 'note',
        '.md': 'note',
        '.pdf': 'pdf',
        '.mp3': 'audio',
        '.mp4': 'video',
        '.docx': 'docx',
        '.pptx': 'ppt',
    }
    
    @staticmethod
    def scan_directory(directory: Path, processed_paths: Set[str]) -> List[Path]:
        """
        Recursively scan directory for supported files (DFS traversal).
        
        Optimized: Check extension first, resolve only if needed
        
        Args:
            directory: Path object of directory to scan
            processed_paths: Set of already-processed absolute paths (mutated in-place)
        
        Returns:
            List of Path objects for found files
        """
        found_files: List[Path] = []
        
        if directory is None or not directory.exists():
            if logger.isEnabledFor(logging.WARNING):
                logger.warning(f"Directory does not exist: {directory}")
            return found_files
        
        if not directory.is_dir():
            if logger.isEnabledFor(logging.WARNING):
                logger.warning(f"Path is not a directory: {directory}")
            return found_files
        
        try:
            for item in directory.iterdir():
                try:
                    if item.is_dir():
                        found_files.extend(
                            FileUtil.scan_directory(item, processed_paths)
                        )
                    elif item.is_file():
                        ext = item.suffix.lower() if item.suffix else ""
                        
                        if ext not in FileUtil.SUPPORTED_EXTENSIONS:
                            continue
                        
                        path_str = str(item.resolve())
                        
                        if path_str in processed_paths:
                            continue
                        
                        found_files.append(item)
                        processed_paths.add(path_str)
                        
                        if len(found_files) % 100 == 0:
                            if logger.isEnabledFor(logging.DEBUG):
                                logger.debug(f"Found {len(found_files)} files so far...")
                
                except PermissionError:
                    if logger.isEnabledFor(logging.WARNING):
                        logger.warning(f"Permission denied: {item}")
                    continue
        
        except PermissionError:
            if logger.isEnabledFor(logging.WARNING):
                logger.warning(f"Cannot access directory: {directory}")
        
        return found_files
    
    @staticmethod
    def scan_directory_parallel(
        directory: Path, 
        processed_paths: Set[str],
        max_workers: int = 4
    ) -> List[Path]:
        """
        Parallel directory scanning using threads (2-4Ã— faster for large trees).
        
        Scans top-level subdirectories in parallel, then aggregates results.
        Best for: Large nested directory structures (10K+ files, 50+ subdirs).
        
        Args:
            directory: Root directory to scan
            processed_paths: Set of already-processed paths (mutated in-place)
            max_workers: Number of threads (default: 4, typically optimal)
        
        Returns:
            List of Path objects for found files
        """
        found_files: List[Path] = []
        
        if directory is None or not directory.exists() or not directory.is_dir():
            if logger.isEnabledFor(logging.WARNING):
                logger.warning(f"Invalid directory: {directory}")
            return found_files
        
        try:
            # Get top-level items
            items = list(directory.iterdir())
            subdirs = [item for item in items if item.is_dir()]
            files = [item for item in items if item.is_file()]
            
            # Process files in current directory (single-threaded, fast)
            for item in files:
                try:
                    ext = item.suffix.lower() if item.suffix else ""
                    
                    if ext not in FileUtil.SUPPORTED_EXTENSIONS:
                        continue
                    
                    path_str = str(item.resolve())
                    
                    if path_str not in processed_paths:
                        found_files.append(item)
                        processed_paths.add(path_str)
                
                except PermissionError:
                    continue
            
            # Parallel scan subdirectories (I/O-bound, benefits from threads)
            if subdirs:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # Each subdir gets its own thread
                    futures = [
                        executor.submit(FileUtil.scan_directory, subdir, processed_paths)
                        for subdir in subdirs
                    ]
                    
                    # Collect results
                    for future in futures:
                        try:
                            found_files.extend(future.result())
                        except Exception as e:
                            if logger.isEnabledFor(logging.ERROR):
                                logger.error(f"Error in parallel scan: {e}")
        
        except PermissionError:
            if logger.isEnabledFor(logging.WARNING):
                logger.warning(f"Cannot access directory: {directory}")
        
        return found_files
    
    @staticmethod
    def get_file_extension(file_path: Path) -> str:
        """Get file extension in lowercase (includes dot)."""
        return file_path.suffix.lower() if file_path.suffix else ""
    
    @staticmethod
    def determine_type(file_path: Path) -> str:
        """Determine item type based on file extension."""
        ext = file_path.suffix.lower() if file_path.suffix else ""
        return FileUtil.TYPE_MAPPING.get(ext, 'unknown')
    
    @staticmethod
    def is_supported_file(file_path: Path) -> bool:
        """Check if file extension is supported."""
        ext = file_path.suffix.lower() if file_path.suffix else ""
        return ext in FileUtil.SUPPORTED_EXTENSIONS
    
    @staticmethod
    def get_file_stats(file_path: Path) -> dict:
        """Get file metadata (size, modified time, etc.)."""
        try:
            stat_info = file_path.stat()
            return {
                'size_bytes': stat_info.st_size,
                'modified_time': stat_info.st_mtime,
                'is_readable': stat_module.S_ISREG(stat_info.st_mode),
            }
        except OSError as e:
            if logger.isEnabledFor(logging.ERROR):
                logger.error(f"Cannot get stats for {file_path}: {e}")
            return {
                'size_bytes': 0,
                'modified_time': 0,
                'is_readable': False,
            }
    
    @staticmethod
    def get_supported_extensions_list() -> List[str]:
        """Get supported extensions as list."""
        return sorted(FileUtil.SUPPORTED_EXTENSIONS)